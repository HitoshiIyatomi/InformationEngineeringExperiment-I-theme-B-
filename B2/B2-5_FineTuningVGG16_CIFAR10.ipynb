{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"}},"cells":[{"cell_type":"markdown","metadata":{"id":"tfPPYzk-8TFP"},"source":["学生証番号 :\n","\n","名前 :\n","\n","メールアドレス(法政大学) :"]},{"cell_type":"markdown","metadata":{"id":"4cSzfJmxM9_J"},"source":["# B2-5 VGG16ネットワークを用いた学習(2) 転移学習、Fine-tuning\n","\n","拡大したCIFAR10画像をVGGネットワークの構造で学習する。\n","\n","B2-4の実験では、たった数回の学習でもとても時間がかかった。\n","これは深層学習モデルのパラメータがとても多く、計算量が大きいからである。\n","\n","今回は、事前学習により得られた重み（パラメータ）を効果的に活用する転移学習と呼ばれる、実践的な学習法を体験する。\n","\n","転移学習では、すでに得られている重み（今回だと画像中の特徴を捉える画像フィルタ）をそのまま活用し、一部分のみ再学習する。\n","\n","\n","具体的には、本当の目的（CIFAR10)とは異なるImageNetで学習された重みを活用し、最後の12800$\\times$10の部分のみを学習する。\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1G0WQ5MaVpHRCktqHBiGbHARPb_MlGIDd\" width = 50%></img>"]},{"cell_type":"code","metadata":{"id":"1mKiDkeaM6eH"},"source":["# [1-0]\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.applications import vgg16"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LUWdjojmRUSq"},"source":["## データセットの準備"]},{"cell_type":"code","metadata":{"id":"6-nQwWW9RY6p"},"source":["# [1-1]\n","def get_data(data_name=\"CIFAR10\"):\n","    if data_name == \"CIFAR100\":\n","        (X_train, y_train), (X_test, y_test) = keras.datasets.cifar100.load_data()\n","    else:\n","        (X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n","    y_train = y_train.flatten()\n","    y_test = y_test.flatten()\n","    return (X_train, y_train), (X_test, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qfjl5X1eCbSb"},"source":["# [1-2]\n","# 予測ラベルの名前を所持しておく\n","target_names = [\n","    \"airplane\",\n","    \"automobile\",\n","    \"bird\",\n","    \"cat\",\n","    \"deer\",\n","    \"dog\",\n","    \"frog\",\n","    \"horse\",\n","    \"ship\",\n","    \"truck\",\n","]\n","\n","\n","def random_plot(X, y, predict=None):\n","    W = 10\n","    H = 5\n","    fig = plt.figure(figsize=(10, 20))\n","    fig.subplots_adjust(left=0, right=1, bottom=0, top=0.3, hspace=0.10, wspace=0.10)\n","    for i in range(5):\n","        for j in range(10):\n","            x_tmp = X[y == j]\n","            idx = np.random.randint(len(x_tmp))\n","            x = x_tmp[idx].reshape(32, 32, 3)\n","            ax = fig.add_subplot(H, W, (i * 10) + j + 1, xticks=[], yticks=[])\n","            ax.imshow(x)\n","            if predict is not None:\n","                pred_tmp = predict[y == j]\n","                p = pred_tmp[idx]\n","                ax.set_title(f\"{j} -> {p}\")\n","            else:\n","                ax.set_title(target_names[j])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kDcD9hcsNpv0"},"source":["# [1-3]\n","DATASET = \"CIFAR10\"\n","# CIFAR100を使用したい場合は以下をコメントアウト\n","# CIFAR100の場合はrandom_plotが全てのラベルを表示しないので注意\n","# DATASET = \"CIFAR100\"\n","(X_train, y_train), (X_test, y_test) = get_data(DATASET)\n","\n","print(\"X_train shape:\", X_train.shape)\n","print(\"y_train shape:\", y_train.shape)\n","print(\"X_test shape:\", X_test.shape)\n","print(\"y_test shape:\", y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Dv1adWUCf9P"},"source":["# [1-4]\n","# 学習用データを覗いてみる。\n","# 複数回実行すると画像も変化する。\n","random_plot(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KB98zjcRRHeD"},"source":["## 学習済みのVGG16ネットワークのダウンロード\n","\n","B02-2と同様に、ImageNetで学習済みのネットワークをダウンロードする。\n","include_top=Falseとして、全結合層の部分を取り外している。  \n","つまり、Conv5_3をmax poolしたところまでのネットワークになっている。\n","\n","B02-3と、まったく同じ構造だが、前回との違いは、今回のモデルが**学習済み**であることである。\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Cf9gcnNIOBZC"},"source":["# [1-5]\n","# VGG16をCIFAR10ように改良したモデルを読み込む関数を定義\n","def get_vgg16_model(data_name=\"CIFAR10\"):\n","    num_output = 10 if data_name == \"CIFAR10\" else 100\n","    # change input image size 224x224 -> 160x160\n","    # conv5_3のあとのmax poolまでのネットワーク(core)の出力をflattenした後、\n","    # 全結合層を介して10次元の出力をつなげる\n","    model = keras.Sequential(\n","        [\n","            keras.Input(shape=(None, None, 3)),\n","            keras.layers.Lambda(\n","                lambda img: tf.image.resize(img, (160, 160)), name=\"resize\"\n","            ),\n","            keras.layers.Lambda(vgg16.preprocess_input, name=\"preprocess\"),\n","            # VGGのCNN部分\n","            vgg16.VGG16(include_top=False, weights=\"imagenet\"),\n","            keras.layers.Flatten(),\n","            keras.layers.Dense(num_output, activation=\"softmax\"),\n","        ]\n","    )\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M2qP6mAqCnVN"},"source":["# [1-6]\n","# モデルの読みこみ\n","model = get_vgg16_model(DATASET)\n","\n","# モデルの全体図の概要\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L-l_mwy5CoQX"},"source":["# [1-7]\n","# VGG部分の概要確認\n","model.layers[2].summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X1qxjRsLYIBh"},"source":["## モデルセットアップと重みの固定\n","\n","今までのmodel_setup関数を改良し、学習する層を変更できるようにしてある。\n","今回は全結合層(dense)部分のみ学習を行うため、training引数には\"dense\"を指定する。\n","学習される重みの数を確認し、B2-4で行ったゼロからの学習（scratch学習）の時と比較せよ。\n","\n","学習の対象重みの数： (12800 + bias 1) x 10 (output)"]},{"cell_type":"code","metadata":{"id":"HDcGczOmDZyz"},"source":["# [1-8]\n","# 誤差関数と最適化手法を設定する関数を作成する。\n","# 新しく、学習する層を指定するtrainingという引数を用意。\n","def model_setup(model, training=\"all\", optimizer=\"adam\", lr=0.001):\n","    if optimizer == \"adam\":\n","        optim = keras.optimizers.Adam(learning_rate=lr)\n","    elif optimizer == \"sgd\":\n","        optim = keras.optimizers.SGD(learning_rate=lr)\n","    else:\n","        raise ValueError\n","    # 学習する層の選択\n","    if training == \"dense\":\n","        model.layers[2].trainable = False\n","    elif training == \"deep\":\n","        for i in range(15):\n","            model.layers[2].layers[i].trainable = False\n","    model.compile(\n","        loss=\"sparse_categorical_crossentropy\", optimizer=optim, metrics=[\"accuracy\"]\n","    )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J9sSymX9YHxE"},"source":["# [1-9]\n","# denseの部分だけ学習するときはtrainingに\"dense\"を指定する。\n","model_setup(model, training=\"dense\")\n","# 学習されるパラメータの数を確認しておく。\n","# （これでも大規模な全結合層　flatten-4096-4096-1000 を外しているので総数は普通のVGG16 の1/10程度になっている）\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k5L57Kj5YzbL"},"source":["# VGG16の事前学習の重み使用した転移学習\n","最初に学習前のモデルで予測を行い、性能を確認する。"]},{"cell_type":"code","metadata":{"id":"ogK29to2YwbL"},"source":["# [1-10]\n","# テストデータで評価する\n","loss_no_train, accuracy_no_train = model.evaluate(X_test, y_test, verbose=0)\n","print(\"学習前の誤差 : \", loss_no_train)\n","print(\"学習前の正解率 : \", accuracy_no_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XIqcFwPrEKzZ"},"source":["予測確率を出すdense部分が学習できていないため、ランダムな予測と同じ結果になる。"]},{"cell_type":"markdown","metadata":{"id":"aom5pzluZX60"},"source":["## モデルの学習\n","B2-3 の全体を学習するスクラッチ学習と比べ学習するパラメータが少ない分、学習時間を短縮できるが、スクラッチと同様に学習には時間がかかるため、実験時間内では3 epochのみの学習しか行わない。 学習の傾向に関しては事前に実行した時のログから確認する。\n","\n","※ レポートには10epochで学習したモデルの結果のグラフや、予測などを確認してほしいので授業外でEPOCHの値を10にして全体を実行しなおすこと。すべて実行完了するまでに１時間ほどかかるが、ヘッダーの「ランタイム」タブの「再起動してすべてのセルを実行」を行うことで全体を実行してくれる。\n","１時間放置していると自動でセッションが切れてしまうので、定期的に確認すること。"]},{"cell_type":"code","metadata":{"id":"6JKMw3tCYsTF"},"source":["# [1-11]\n","EPOCH = 3\n","# ここを10にして再度実行する。\n","# EPOCH = 10\n","batch_size = 128\n","\n","trainlog = model.fit(\n","    X_train, y_train, epochs=EPOCH, validation_split=0.1, batch_size=batch_size\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AYb45RdDIdQo"},"source":["以下が10EPOCH回した時のログになっている。\n","```\n","```\n","転移学習を使用することで1EPOCHめから高い性能で予測できていることが確認できた。\n","また、途中からtrainとvalidationの性能の差が大きくなり、過学習してしまっていることも確認できた。"]},{"cell_type":"code","metadata":{"id":"qMwmKTdWZdx8"},"source":["# [1-12]\n","# 学習曲線を描画する関数の作成\n","def plot_result(log):\n","    fig = plt.figure(figsize=(10, 3))\n","    ax1 = fig.add_subplot(1, 2, 1)\n","    ax1.plot(log.history[\"accuracy\"])\n","    ax1.plot(log.history[\"val_accuracy\"])\n","    ax1.set_title(\"Model accuracy\")\n","    ax1.set_ylabel(\"Accuracy\")\n","    ax1.set_xlabel(\"Epoch\")\n","    ax1.legend([\"Train\", \"Validation\"], loc=\"best\")\n","    ax2 = fig.add_subplot(1, 2, 2)\n","    ax2.plot(log.history[\"loss\"])\n","    ax2.plot(log.history[\"val_loss\"])\n","    ax2.set_title(\"Model loss\")\n","    ax2.set_ylabel(\"Loss\")\n","    ax2.set_xlabel(\"Epoch\")\n","    ax2.legend([\"Train\", \"Validation\"], loc=\"best\")\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8KfF6l3xqmSE"},"source":["# [1-13]\n","# 実際に学習曲線の確認\n","# 右が正解率を左が誤差の推移を表している。\n","plot_result(trainlog)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P9mXLdvdsCvM"},"source":["# [1-14]\n","# 評価\n","loss_train, accuracy_train = model.evaluate(X_test, y_test, verbose=0)\n","\n","# 学習前のスコアの表示\n","print(\"学習前の誤差 : \", loss_no_train)\n","print(\"学習前の正解率 : \", accuracy_no_train)\n","\n","# 学習後のスコアの表示\n","print(\"===== 学習後 =====\")\n","print(f\"学習後の誤差 :  \", loss_train)\n","print(f\"学習後の正解率 : \", accuracy_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vLouBdokcUVb"},"source":["---\n","## 学習する部分をより多くした　Fine-tuning\n","\n","事前学習した重みを初期値として用い、事前学習した部分も学習することをFine-tuningと呼ぶ。\n","Fine-tuningは、全部の重みを更新することも含めるが、それに限らない。\n","\n","今回は、Conv5の畳み込み層(block5_conv1 ～ block5_conv3)も含めて再学習（fine-tuning)を行い、精度を検証する。\n","\n","\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=11UF8Y7XxJYoAB2U53iishzZHGGiBeLbV\" width = 50%></img>\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mDYOImI3PhwT"},"source":["先ほどのTransfer learningのモデルは85-87%程度で性能の限界が来た。\n","途中までは先ほどと同様。新しくモデルを定義する。(model 2)\n"]},{"cell_type":"code","metadata":{"id":"3g7m-Zg17t2C"},"source":["# [2-0]\n","# 先ほどと同様のnetwork をmodel2として再定義\n","model2 = get_vgg16_model(DATASET)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UMmTU7m-40H0"},"source":["## モデルのセットアップ\n","FineTuningの場合は事前学習で得られた初期値を変更しすぎないように小さい学習率を用いる。"]},{"cell_type":"code","metadata":{"id":"l2TV04EFcTpE"},"source":["# [2-1]\n","# 深いCNNの層とdenseの部分だけ学習するときはtrainingに\"deep\"を指定する。\n","model_setup(model2, training=\"deep\", lr=0.00005)\n","# 学習する部分のパラメータ確認\n","model2.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qApvW9nO8McG"},"source":["## モデルの学習\n","先ほどと同様に3 EPOCH学習する。\n","※こちらも上記と同様に授業外で10EPOCHの学習を行う。"]},{"cell_type":"code","metadata":{"id":"vkb9x83NdpVD"},"source":["# [2-2]\n","EPOCH = 3\n","# ここを10にして再度実行する。\n","# EPOCH = 10\n","batch_size = 128\n","\n","trainlog2 = model2.fit(\n","    X_train, y_train, epochs=EPOCH, validation_split=0.1, batch_size=batch_size\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UHs-0OZCIs-D"},"source":["（参考）以下が10epoch学習を行った時のログ。レポートでは必ず自分の実施データに基づいた結果の報告及び考察を行うこと。\n","```\n","Epoch 1/10\n","352/352 [==============================] - 217s 615ms/step - loss: 1.0685 - accuracy: 0.6766 - val_loss: 0.4877 - val_accuracy: 0.8380\n","Epoch 2/10\n","352/352 [==============================] - 216s 613ms/step - loss: 0.3255 - accuracy: 0.8915 - val_loss: 0.3742 - val_accuracy: 0.8744\n","Epoch 3/10\n","352/352 [==============================] - 216s 613ms/step - loss: 0.1394 - accuracy: 0.9559 - val_loss: 0.3495 - val_accuracy: 0.8886\n","Epoch 4/10\n","352/352 [==============================] - 216s 614ms/step - loss: 0.0524 - accuracy: 0.9861 - val_loss: 0.3654 - val_accuracy: 0.8944\n","Epoch 5/10\n","352/352 [==============================] - 216s 614ms/step - loss: 0.0166 - accuracy: 0.9974 - val_loss: 0.3722 - val_accuracy: 0.9018\n","Epoch 6/10\n","352/352 [==============================] - 216s 614ms/step - loss: 0.0053 - accuracy: 0.9998 - val_loss: 0.3918 - val_accuracy: 0.9036\n","Epoch 7/10\n","352/352 [==============================] - 216s 614ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.4146 - val_accuracy: 0.9066\n","Epoch 8/10\n","352/352 [==============================] - 216s 614ms/step - loss: 9.1993e-04 - accuracy: 1.0000 - val_loss: 0.4433 - val_accuracy: 0.9078\n","Epoch 9/10\n","352/352 [==============================] - 216s 614ms/step - loss: 5.9912e-04 - accuracy: 1.0000 - val_loss: 0.4580 - val_accuracy: 0.9078\n","Epoch 10/10\n","352/352 [==============================] - 216s 614ms/step - loss: 4.2908e-04 - accuracy: 1.0000 - val_loss: 0.4702 - val_accuracy: 0.9090\n","```\n","畳み込み層を全て固定している転移学習よりもvalidationデータに対して高い性能が確認できた。"]},{"cell_type":"code","metadata":{"id":"fb75pTLjFVsn"},"source":["# [2-3]\n","# 実際に学習曲線の確認\n","# 右が正解率を左が誤差の推移を表している。\n","plot_result(trainlog2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zG_LWZnMA_oU"},"source":["# [2-4]\n","# 評価\n","loss_train_ft, accuracy_train_ft = model2.evaluate(X_test, y_test, verbose=0)\n","\n","# 学習前のスコアの表示\n","print(\"転移学習の誤差 : \", loss_train)\n","print(\"転移学習の正解率 : \", accuracy_train)\n","\n","# 学習後のスコアの表示\n","print(\"===== 学習後 =====\")\n","print(f'Fine Tuning後の誤差 :  ', loss_train_ft)\n","print(f'Fine Tuning後の正解率 : ', accuracy_train_ft)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HAZcr0jJRxfx"},"source":["独自の追加実験などは、以下のセルで行え。（いくつつかってもいい）\n","ただし、一度学習したモデルを再度学習すると、追加の学習になってしまうので注意。\n"]},{"cell_type":"code","metadata":{"id":"5JSXHASxRwSC"},"source":["# ここから実験を追加する。(自由にセルを増やしてOK)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oo0UMzsORfdS"},"source":["## 課題B2-5 転移学習やFineTuningと、それを行わなかった場合との比較\n","\n","事前学習モデルを使わなかったとき（B2-4)と、今回の実験で転移学習やFine tuning、また独自に追加した異なる条件で行った実験により得られた結果を比較し、学習されるパラメータ数、学習時間、精度について考察せよ。\n","\n","**（転移学習やFine-tuningを行った場合と、そうでない場合にどんな違いがあるかの方が重要。その上で、再学習する層の数の違いに着目する余裕があれば行う。）**\n","\n","転移学習やFine-tuningを行うと、どんないいことがあるのか。またそれはいつでも行えるのか。行うための条件などがあるのか議論せよ。\n"]}]}