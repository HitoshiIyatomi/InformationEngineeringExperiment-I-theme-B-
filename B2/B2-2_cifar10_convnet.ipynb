{"cells":[{"cell_type":"markdown","metadata":{"id":"ssOxNiruPKWA"},"source":["学生証番号 :\n","\n","名前 :\n","\n","メールアドレス(法政大学) :"]},{"cell_type":"markdown","metadata":{"id":"NIZgWgq_IdLD"},"source":["# B2-2 CNNの畳み込み層の深さと識別能\n","\n","--- 自分で層を追加してみることの体験\n","\n","畳み込みニューラルネットワーク（CNN)を用いた識別において、ネットワークの深さを含めた構造を変えることにより精度に与える影響を検討する。\n","\n","B2-1では、5x5の畳み込みを用いていたが、今回は3x3の畳み込みを行うCNNで比較を行う。\n","\n","大きい畳み込みフィルタは、大きい領域の局所領域を獲得できるが、計算量が大きい。  \n","例えば7x7の畳み込みは3x3の畳み込み3回に分割できる。\n","\n","<img src=\"https://drive.google.com/uc?export=view&id=1MiFvpoHonF33q9-9HqpLybN7ZTNEHNEi\" width = 50%></img>\n","\n","この方がパラメータの数（ここだと49 v.s. 3x3x3)が少なく、学習にかかる重みの更新回数も少なくて済む。(配布資料を参照のこと）\n","モデルのパラメータが少ないことは、過学習抑制の側面から重要である。\n","\n","こうした効率的な多層に亘る畳み込みは、深層学習モデルの過学習を抑えつつ、表現能力を高めることに貢献している。\n"]},{"cell_type":"markdown","metadata":{"id":"0jjR2DwnIdLH"},"source":["## 必要なライブラリのインポートと関数の事前準備"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1QzKQySjIdLI"},"outputs":[],"source":["# [1-0]\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","\n","import matplotlib.pyplot as plt\n","from matplotlib import pyplot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JMxFzufTp3KA"},"outputs":[],"source":["# [1-1]\n","# データを呼び出す関数の作成。\n","# 内容はB2-1のget_data_mnistと同じ\n","def get_data_cifar(data_size=None):\n","    (X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n","    # 0 ~ 1の範囲に正規化を行う\n","    X_train = X_train.astype(np.float32) / 255.0\n","    X_test = X_test.astype(np.float32) / 255.0\n","    y_train = y_train.flatten()\n","    y_test = y_test.flatten()\n","    if data_size is not None:\n","        X_train = X_train[:data_size]\n","        y_train = y_train[:data_size]\n","    return (X_train, y_train), (X_test, y_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"II7rOs-iqH49"},"outputs":[],"source":["# [1-2]\n","# 予測ラベルの名前を所持しておく\n","target_names = [\n","    \"airplane\",\n","    \"automobile\",\n","    \"bird\",\n","    \"cat\",\n","    \"deer\",\n","    \"dog\",\n","    \"frog\",\n","    \"horse\",\n","    \"ship\",\n","    \"truck\",\n","]\n","# B1-2のrandom_plotと同じ\n","def random_plot(X, y, predict=None):\n","    W = 10\n","    H = 5\n","    fig = plt.figure(figsize=(10, 20))\n","    fig.subplots_adjust(left=0, right=1, bottom=0, top=0.3, hspace=0.10, wspace=0.10)\n","    for i in range(5):\n","        for j in range(10):\n","            x_tmp = X[y == j]\n","            idx = np.random.randint(len(x_tmp))\n","            x = x_tmp[idx].reshape(32, 32, 3)\n","            ax = fig.add_subplot(H, W, (i * 10) + j + 1, xticks=[], yticks=[])\n","            ax.imshow(x)\n","            if predict is not None:\n","                pred_tmp = predict[y == j]\n","                p = pred_tmp[idx]\n","                ax.set_title(f\"{j} -> {p}\")\n","            else:\n","                ax.set_title(target_names[j])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZVkF7xAUqNix"},"outputs":[],"source":["# [1-3]\n","# 誤差関数と最適化手法を設定する関数を作成する。\n","def model_setup(model, optimizer=\"adam\", lr=0.001):\n","    if optimizer == \"adam\":\n","        optim = keras.optimizers.Adam(learning_rate=lr)\n","    elif optimizer == \"sgd\":\n","        optim = keras.optimizers.SGD(learning_rate=lr)\n","    else:\n","        raise ValueError\n","    model.compile(\n","        loss=\"sparse_categorical_crossentropy\", optimizer=optim, metrics=[\"accuracy\"]\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"etsHSO5rqRZB"},"outputs":[],"source":["# [1-4]\n","# 学習曲線を描画する関数の作成\n","def plot_result(log):\n","    fig = plt.figure(figsize=(10, 3))\n","    ax1 = fig.add_subplot(1, 2, 1)\n","    ax1.plot(log.history[\"accuracy\"])\n","    ax1.plot(log.history[\"val_accuracy\"])\n","    ax1.set_title(\"Model accuracy\")\n","    ax1.set_ylabel(\"Accuracy\")\n","    ax1.set_xlabel(\"Epoch\")\n","    ax1.legend([\"Train\", \"Validation\"], loc=\"best\")\n","    ax2 = fig.add_subplot(1, 2, 2)\n","    ax2.plot(log.history[\"loss\"])\n","    ax2.plot(log.history[\"val_loss\"])\n","    ax2.set_title(\"Model loss\")\n","    ax2.set_ylabel(\"Loss\")\n","    ax2.set_xlabel(\"Epoch\")\n","    ax2.legend([\"Train\", \"Validation\"], loc=\"best\")\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"LUWdjojmRUSq"},"source":["### データセットの準備\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6-nQwWW9RY6p"},"outputs":[],"source":["# [1-5]\n","(X_train, y_train), (X_test, y_test) = get_data_cifar()\n","\n","print(\"X_train shape:\", X_train.shape)\n","print(\"y_train shape:\", y_train.shape)\n","print(\"X_test shape:\", X_test.shape)\n","print(\"y_test shape:\", y_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kDcD9hcsNpv0"},"outputs":[],"source":["# [1-6]\n","# 学習用データを覗いてみる。\n","# 複数回実行すると画像も変化する。\n","random_plot(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xmemjZt2yQ_X"},"outputs":[],"source":["# [1-7]\n","# テストデータを覗いてみる。\n","random_plot(X_test, y_test)"]},{"cell_type":"markdown","metadata":{"id":"ya7NEsaKIdLJ"},"source":["## モデルの構築と評価\n","畳み込み層1層のモデルを構築する(B2-1のMNISTの時と同じ構成)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HZp_TCFLN2hM"},"outputs":[],"source":["# [1-8]\n","model1 = keras.Sequential(\n","    [\n","        layers.InputLayer(input_shape=(32, 32, 3)),\n","        layers.Conv2D(32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"),\n","        layers.MaxPooling2D(pool_size=(2, 2)),\n","        layers.Flatten(),\n","        layers.Dense(20),\n","        layers.Dense(10, activation=\"softmax\"),\n","    ]\n",")\n","\n","model1.summary()"]},{"cell_type":"markdown","metadata":{"id":"eN3odEAVN2hP"},"source":["## モデルのセットアップと学習"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6fxw-lPyO9xM"},"outputs":[],"source":["# [1-9]\n","# モデルのセットアップ\n","model_setup(model1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WZ9Ut7JWN2hQ"},"outputs":[],"source":["# [1-10]\n","# 比較のための実験条件は固定する\n","batch_size = 128\n","epochs = 10\n","\n","# 学習\n","trainlog_conv1 = model1.fit(\n","    X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1\n",")"]},{"cell_type":"markdown","metadata":{"id":"zSR1V1MbN2hQ"},"source":["### 学習ログの可視化"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zeDbqJMEN2hQ"},"outputs":[],"source":["# [1-11]\n","# 実際に学習曲線の確認\n","# 右が正解率を左が誤差の推移を表している。\n","plot_result(trainlog_conv1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ObvhhqBrN2hQ"},"outputs":[],"source":["# [1-12]\n","# 評価\n","loss_train_conv1, accuracy_train_conv1 = model1.evaluate(X_test, y_test, verbose=0)\n","\n","# 学習後のスコアの表示\n","print(f\"convが１層のときの誤差 :  \", loss_train_conv1)\n","print(f\"convが１層のときの正解率 : \", accuracy_train_conv1)"]},{"cell_type":"markdown","metadata":{"id":"wo5kwglmN8nm"},"source":["MNISTと比べてBPNNよりも性能の差が見られた。これはなぜか？\n","\n","次に畳み込み層を2層に変更したモデルの学習を行う。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jpy3DjlpIdLJ"},"outputs":[],"source":["# [1-13]\n","model2 = keras.Sequential(\n","    [\n","        layers.InputLayer(input_shape=(32, 32, 3)),\n","        layers.Conv2D(32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"),\n","        layers.Conv2D(32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"),\n","        layers.MaxPooling2D(pool_size=(2, 2)),\n","        layers.Flatten(),\n","        layers.Dense(20),\n","        layers.Dense(10, activation=\"softmax\"),\n","    ]\n",")\n","\n","model2.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b8T1z9g2VMg9"},"outputs":[],"source":["# [1-14]\n","# モデルのセットアップ\n","model_setup(model2)\n","# 学習\n","trainlog_conv2 = model2.fit(\n","    X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1\n",")"]},{"cell_type":"markdown","metadata":{"id":"DSIVKdT1VX-S"},"source":["## 学習ログの可視化"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3V9kPKpeMBde"},"outputs":[],"source":["# [1-15]\n","plot_result(trainlog_conv2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"baFKX57bIdLK"},"outputs":[],"source":["# [1-16]\n","# 評価\n","loss_train_conv2, accuracy_train_conv2 = model2.evaluate(X_test, y_test, verbose=0)\n","\n","# 学習後のスコアの表示\n","print(f\"convが１層のときの誤差 :  \", loss_train_conv1)\n","print(f\"convが１層のときの正解率 : \", accuracy_train_conv1)\n","# 学習後のスコアの表示\n","print(\"===== 層の数を増加 =====\")\n","print(f\"convが2層のときの誤差 :  \", loss_train_conv2)\n","print(f\"convが2層のときの正解率 : \", accuracy_train_conv2)"]},{"cell_type":"markdown","metadata":{"id":"XgkkXk6QktDp"},"source":["# 課題B2-2-1 CNNの層の数を変化させた時のパラメータ数と性能の関係\n","CNNの畳み込み層の数を増やした時、またpooling処理の場所を変えたり増減した場合に、学習すべきパラメータの数や、識別能などがどのように変わるか比較・検討せよ。（B02-1 で行った5x5の畳み込みとの比較もしてみよ）\n","また、なぜそういった結果になったのかも考察せよ。\n","\n","ヒント\n","- モデル構造の設定  \n","下記は　input $\\rightarrow$ conv layer x3 $\\rightarrow$ max pooling（ $\\rightarrow$ flatten) $\\rightarrow$ Dense $\\rightarrow$ Dense $\\rightarrow$ output\n","```\n","model = keras.Sequential(\n","    [\n","        layers.InputLayer(input_shape=(32, 32, 3)), # ここは変更しない\n","        # ここから #\n","        layers.Conv2D(32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"),\n","        layers.Conv2D(32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"),\n","        layers.Conv2D(32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"),\n","        layers.MaxPooling2D(pool_size=(2, 2)),\n","        # ここまでをいじる #  \n","        layers.Flatten(),\n","        layers.Dense(20, activation=\"relu\"),    \n","        layers.Dense(10, activation=\"softmax\"), # ここも変更しない\n","    ]\n",")\n","# 定義したモデルを確認\n","model.summary()\n","# モデルセットアップ\n","model_setup(model)\n","```\n","- 学習と評価\n","```\n","batch_size = 128\n","epochs = 10\n","log = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n","# 評価\n","loss, accuracy = model.evaluate(X_test, y_test)\n","print(\"誤差　: \", loss)\n","print(\"正解率 : \", accuracy)\n","```\n","\n","\n"," このnotebookで実験を行い、レポートで議論せよ。"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BETLYrct2bgY"},"outputs":[],"source":["# ここから実験を追加する。(自由にセルを増やしてOK)"]},{"cell_type":"markdown","metadata":{"id":"wKlZrCYgfO7o"},"source":["# 課題B2-2-2 CNNの理解\n","3×3のカーネルを持つCNNを2層連続させたときの出力(feature map)の１つの値(ピクセル)は、入力画像のどの範囲から得られたものであるか？計算して考えよ。"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"https://github.com/keras-team/keras-io/blob/master/examples/vision/ipynb/mnist_convnet.ipynb","timestamp":1620614213048}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.1"}},"nbformat":4,"nbformat_minor":0}